{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d91c455-3627-4030-9948-a4f105d267ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from os.path import splitext\n",
    "from os import listdir\n",
    "from glob import glob\n",
    "import gc\n",
    "import random\n",
    "import logging\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import albumentations as A\n",
    "from torchvision.models.segmentation import fcn_resnet101\n",
    "from torchvision.models.segmentation.fcn import FCNHead\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from scipy.ndimage import morphology\n",
    "\n",
    "from classes import Material, BasicDataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f20be1-6a4e-4f27-aa1b-6440619e3908",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "\"materials\" :[\n",
    "             Material(\"background\", [85,85,85], 30, 0.5),\n",
    "             Material(\"epidermis\", [170,170,170], 150, 0.5),\n",
    "             Material(\"mesophyll\", [255,255,255], 255, 0.5),\n",
    "             Material(\"air_space\", [0,0,0], 1, 0.5),\n",
    "             Material(\"bundle_sheath_extension\", [103,103,103], 100, 0.5),\n",
    "             Material(\"vein\", (35,35,35), 180, 0.5)\n",
    "            ],\n",
    "#Various input/output directories\n",
    "\"training_image_directory\" : \"train/train_images/\",\n",
    "\"training_mask_directory\" : \"train/train_masks/\",\n",
    "#Fraction of total annotations you want to leave for validating the model.\n",
    "\"validation_fraction\": 0.2,\n",
    "#Model Performance varies, make multiple models to have the best chance at success.\n",
    "\"num_models\" : 1,\n",
    "#Model Performance improves with increasing epochs, to a point.\n",
    "\"num_epochs\" : 100,\n",
    "\"batch_size\" : 1,\n",
    "#Decrease scale to decrease VRAM usage; if you run out of VRAM during traing, restart your runtime and down scale your images\n",
    "\"scale\" : 1,\n",
    "\"seed\" : 0,\n",
    "\"models_directory\" : \"best_models/\",\n",
    "\"model_group\" : 'test/',\n",
    "\"current_model_name\" : 'test',\n",
    "\"test_images\" : \"test/test_images/\",\n",
    "\"test_masks\": \"test/test_masks/\",\n",
    "\"csv_directory\" : \"other/\",\n",
    "#Input the directory of the data you want to segment here.\n",
    "\"inference_directory\": \"other/\",\n",
    "#Input the 5 alpha-numeric characters proceding the file number of your images\n",
    "  #EX. Jmic3111_S0_GRID image_0.tif ----->mage_\n",
    "\"proceeding\":\"lice_\",\n",
    "#Input the 4 or mor alpha-numeric characters following the file number\n",
    "  #EX. Jmic3111_S0_GRID image_0.tif ----->.tif\n",
    "\"following\" : \".png\",\n",
    "\"output_directory\": \"out/\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc03126-6c64-43df-84db-62f5b497c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegModel():\n",
    "    def __init__(self, init_dict):\n",
    "        self.__dict__ = init_dict\n",
    "        self.num_materials = len(init_dict[\"materials\"])\n",
    "        self.dir_checkpoint = self.models_directory\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        os.makedirs(self.dir_checkpoint+self.model_group, exist_ok=True )\n",
    "        \n",
    "    def set_up_model(self):\n",
    "        self.model = fcn_resnet101(pretrained=True, progress=True)\n",
    "        self.model.classifier=FCNHead(2048, self.num_materials)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        return self.model\n",
    "    \n",
    "    def get_data(self):\n",
    "        return BasicDataset(self.training_image_directory, self.training_mask_directory, self.materials, scale=self.scale, transform=False)\n",
    "    \n",
    "    def get_loader(self, dataset):\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    def trainval_split(self, dataset):\n",
    "        validation_size = int(len(dataset) * self.validation_fraction)\n",
    "        train_size = len(dataset) - validation_size\n",
    "        train, val = torch.utils.data.random_split(dataset, [train_size, validation_size], generator=torch.Generator().manual_seed(self.seed))\n",
    " \n",
    "        return train, val\n",
    "\n",
    "    def setup_data(self):\n",
    "        dataset = self.get_data()\n",
    "        train_loader = self.get_loader(dataset)\n",
    "        nimages = 0\n",
    "        mean = 0. \n",
    "        std = 0.\n",
    "        for batch, _ in train_loader:\n",
    "            # Rearrange batch to be the shape of [B, C, W * H]\n",
    "            batch = batch.view(batch.size(0), batch.size(1), -1)\n",
    "            # Update total number of images\n",
    "            nimages += batch.size(0)\n",
    "            # Compute mean and std here\n",
    "            mean += batch.mean(2).sum(0) \n",
    "            std += batch.std(2).sum(0)\n",
    "\n",
    "        # Final step\n",
    "        mean /= nimages\n",
    "        std /= nimages\n",
    "\n",
    "        print(mean)\n",
    "        print(std)\n",
    "\n",
    "        dataset.means=mean\n",
    "        dataset.stds=std \n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.get_loader(self.dataset_train)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.get_loader(self.dataset_val)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.get_loader(self.dataset_val)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def backward(self, loss, optimizer, optimizer_idx):\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img, mask = batch\n",
    "        pred = self.model(img)\n",
    "        loss = self.criterion(pred, mask)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img, mask = batch\n",
    "        pred = self.model(img)\n",
    "        loss = self.criterion(pred, mask)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        img, mask = batch\n",
    "        pred = self.model(img)\n",
    "        loss = self.criterion(pred, mask)\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        img, mask = batch\n",
    "        pred = self.model(img)\n",
    "        return pred\n",
    "    \n",
    "    def train(self,verbose = True):\n",
    "        self.model = self.set_up_model() \n",
    "        self.dataset = self.get_data()\n",
    "        self.dataset_train, self.dataset_val = self.trainval_split(self.dataset)\n",
    "\n",
    "        self.train_loader = self.get_loader(self.dataset_train)\n",
    "        self.val_loader = self.get_loader(self.dataset_val)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        num_epochs= self.num_epochs\n",
    "        optimizer = self.configure_optimizers()\n",
    "\n",
    "        best_loss=999\n",
    "\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        #this is the train loop\n",
    "        for epoch in range(num_epochs):\n",
    "            #print(psutil.virtual_memory().percent)\n",
    "            if verbose:\n",
    "                print('Epoch: ', str(epoch))\n",
    "        #add back if doing fractional training\n",
    "            self.train_loader.dataset.dataset.transform=True\n",
    "            self.model.train()\n",
    "            for images, masks in self.train_loader:\n",
    "\n",
    "                images = images.to(device=self.device, dtype=torch.float32)\n",
    "                masks = masks.to(device=self.device, dtype=torch.float32)\n",
    "\n",
    "                #forward pass\n",
    "                preds=self.model(images)['out'].cuda()\n",
    "\n",
    "                #compute loss\n",
    "                loss=criterion(preds, masks)\n",
    "\n",
    "                #reset the optimizer gradients to 0\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                #backward pass (compute gradients)\n",
    "                loss.backward()\n",
    "\n",
    "                #use the computed gradients to update model weights\n",
    "                optimizer.step()\n",
    "\n",
    "            if verbose:\n",
    "                print('Train loss: '+str(loss.to('cpu').detach()))\n",
    "\n",
    "        self.val_loader.dataset.dataset.transform=False\n",
    "        current_loss=0\n",
    "\n",
    "        #test on val set and save the best checkpoint\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, masks in self.val_loader:\n",
    "                images = images.to(device=self.device, dtype=torch.float32)\n",
    "                masks = masks.to(device=self.device, dtype=torch.float32)\n",
    "                preds = self.model(images)['out'].cuda()\n",
    "\n",
    "                loss = criterion(preds, masks)\n",
    "                current_loss+=loss.to('cpu').detach()\n",
    "                del images, masks, preds, loss\n",
    " \n",
    "        if best_loss>current_loss:\n",
    "            best_loss=current_loss\n",
    "            print('Best Model Saved!, loss: '+ str(best_loss))\n",
    "            torch.save(self.model.state_dict(), self.dir_checkpoint + self.model_group + self.current_model_name+\".pth\")\n",
    "        else:\n",
    "            print('Model is bad!, Current loss: '+ str(current_loss) + ' Best loss: '+str(best_loss))\n",
    "            print('\\n')\n",
    "    def validation(self): \n",
    "        prop_list = []\n",
    "        for mat in self.materials:\n",
    "            prop_list.append([[],[],[],[]])\n",
    "\n",
    "        for images, target in self.val_loader:\n",
    "            images = images.to(device=self.device, dtype=torch.float32)\n",
    "            target = target.to(device=self.device, dtype=torch.float32)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred=self.model(images)['out'].cuda()\n",
    "                pred=nn.Sigmoid()(pred)\n",
    "\n",
    "            for i, mat in enumerate(self.materials):\n",
    "                material_target = target[:,i,:,:]\n",
    "                material_pred = pred[:, i, :, :]\n",
    "                material_pred[material_pred >=mat.confidence_threshold] = 1\n",
    "                material_pred[material_pred <=mat.confidence_threshold] = 0\n",
    "                pred[:, i, :, :]=material_pred\n",
    "\n",
    "                material_tp=torch.sum(material_target*material_pred, (1,2))\n",
    "                material_fp=torch.sum((1-material_target)*material_pred, (1,2))\n",
    "                material_fn=torch.sum(material_target*(1-material_pred), (1,2))\n",
    "                material_tn=torch.sum((1-material_target)*(1-material_pred), (1,2))\n",
    "\n",
    "                material_precision=torch.mean((material_tp+0.000000001)/(material_tp+material_fp+0.000000001))\n",
    "                material_recall=torch.mean((material_tp+0.000000001)/(material_tp+material_fn+0.000000001))\n",
    "                material_accuracy=torch.mean((material_tp+material_tn+0.000000001)/(material_tp+material_tn+material_fp+material_fn+0.000000001))\n",
    "                material_f1=torch.mean(((material_tp+0.000000001))/(material_tp++0.000000001+0.5*(material_fp+material_fn)))\n",
    "\n",
    "                prop_list[i][0].append(material_precision.cpu().detach().numpy())\n",
    "                prop_list[i][1].append(material_recall.cpu().detach().numpy())\n",
    "                prop_list[i][2].append(material_accuracy.cpu().detach().numpy())\n",
    "                prop_list[i][3].append(material_f1.cpu().detach().numpy())\n",
    "\n",
    "        properties = {\"name\" : [mat.name for mat in self.materials],\n",
    "                \"precision\" : [str(np.mean(prop_list[i][0])) for i in range(self.num_materials)],\n",
    "                \"recall\" : [str(np.mean(prop_list[i][1])) for i in range(self.num_materials)],\n",
    "                \"accuracy\" : [str(np.mean(prop_list[i][2])) for i in range(self.num_materials)],\n",
    "                \"f1\" : [str(np.mean(prop_list[i][3])) for i in range(self.num_materials)]}\n",
    "        self.modeldata = pd.DataFrame(properties, columns = [\"name\", \"precision\", \"recall\", \"accuracy\", \"f1\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd93375a-b480-458e-a5c4-96ddf91009e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Train loss: tensor(0.1634)\n",
      "Epoch:  1\n",
      "Train loss: tensor(0.1381)\n",
      "Epoch:  2\n",
      "Train loss: tensor(0.1311)\n",
      "Epoch:  3\n",
      "Train loss: tensor(0.1219)\n",
      "Epoch:  4\n",
      "Train loss: tensor(0.1494)\n",
      "Best Model Saved!, loss: tensor(4.2514)\n",
      "Epoch:  0\n",
      "Train loss: tensor(0.1693)\n",
      "Epoch:  1\n",
      "Train loss: tensor(0.1390)\n",
      "Epoch:  2\n",
      "Train loss: tensor(0.1708)\n",
      "Epoch:  3\n",
      "Train loss: tensor(0.1475)\n",
      "Epoch:  4\n",
      "Train loss: tensor(0.1258)\n",
      "Best Model Saved!, loss: tensor(2.6964)\n",
      "Epoch:  0\n",
      "Train loss: tensor(0.1659)\n",
      "Epoch:  1\n",
      "Train loss: tensor(0.1527)\n",
      "Epoch:  2\n",
      "Train loss: tensor(0.1554)\n",
      "Epoch:  3\n",
      "Train loss: tensor(0.1194)\n",
      "Epoch:  4\n",
      "Train loss: tensor(0.1191)\n",
      "Best Model Saved!, loss: tensor(3.3628)\n",
      "Epoch:  0\n",
      "Train loss: tensor(0.1680)\n",
      "Epoch:  1\n",
      "Train loss: tensor(0.1510)\n",
      "Epoch:  2\n",
      "Train loss: tensor(0.1427)\n",
      "Epoch:  3\n",
      "Train loss: tensor(0.1180)\n",
      "Epoch:  4\n",
      "Train loss: tensor(0.1267)\n",
      "Best Model Saved!, loss: tensor(8.9043)\n",
      "Epoch:  0\n",
      "Train loss: tensor(0.1499)\n",
      "Epoch:  1\n",
      "Train loss: tensor(0.2032)\n",
      "Epoch:  2\n",
      "Train loss: tensor(0.1431)\n",
      "Epoch:  3\n",
      "Train loss: tensor(0.1572)\n",
      "Epoch:  4\n",
      "Train loss: tensor(0.1636)\n",
      "Best Model Saved!, loss: tensor(6.1492)\n",
      "Epoch:  0\n",
      "Train loss: tensor(0.1939)\n",
      "Epoch:  1\n",
      "Train loss: tensor(0.1428)\n",
      "Epoch:  2\n",
      "Train loss: tensor(0.1242)\n",
      "Epoch:  3\n",
      "Train loss: tensor(0.1578)\n",
      "Epoch:  4\n",
      "Train loss: tensor(0.1192)\n",
      "Best Model Saved!, loss: tensor(5.8045)\n",
      "Epoch:  0\n",
      "Train loss: tensor(0.1807)\n",
      "Epoch:  1\n",
      "Train loss: tensor(0.1470)\n",
      "Epoch:  2\n",
      "Train loss: tensor(0.1325)\n",
      "Epoch:  3\n",
      "Train loss: tensor(0.1244)\n",
      "Epoch:  4\n",
      "Train loss: tensor(0.1393)\n",
      "Best Model Saved!, loss: tensor(4.2569)\n",
      "Epoch:  0\n",
      "Train loss: tensor(0.1759)\n",
      "Epoch:  1\n",
      "Train loss: tensor(0.1438)\n",
      "Epoch:  2\n",
      "Train loss: tensor(0.1191)\n",
      "Epoch:  3\n",
      "Train loss: tensor(0.1183)\n",
      "Epoch:  4\n",
      "Train loss: tensor(0.1584)\n",
      "Best Model Saved!, loss: tensor(6.2025)\n",
      "1min 9s ± 857 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "Leaf = SegModel(inputs)\n",
    "Leaf.num_epochs = 5\n",
    "Leaf.train(verbose = True)\n",
    "Leaf.validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20c9a7c8-9f5f-4e5d-8d57-8728a42d2ecc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Leaf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mLeaf\u001b[49m\u001b[38;5;241m.\u001b[39mmodeldata\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Leaf' is not defined"
     ]
    }
   ],
   "source": [
    "Leaf.modeldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044030a7-67d6-4fc4-b797-6978c0450f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myPyTorch",
   "language": "python",
   "name": "mypytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
